<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <title>SYS2</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
    <link rel="icon" href="images/icon.png">
</head>
<body>
    <div class="container"> <!-- divs can be used to apply styles (in css) to sections of code-->
        <div class="nav-wrapper">
            <div class="left-side">
                <div class="nav-link-wrapper">
                    <a href="index.html">Home</a>
                   
                </div>
                <div class="nav-link-wrapper">
                    <a href="about.html">About</a>
                </div>
                <div class="nav-link-wrapper">
                    <a href="cs_degree.html">CS Degree Stuff</a>
                </div>
            </div>
            <div class="right-side">
                <div class="brand">
                    <div> JONATHAN DAVIES</div> <!-- By themselves, div's only function is a line break-->
                </div>
            </div>
        </div>
        <div class="content-wrapper">
            <div class="sidenav">
                <a href="#os">Operating Systems</a>
                <a href="#process">Processes</a>
                <a href="#ipc">Interprocess Communication</a>
                <a href="#thread">Threads</a>
                <a href="#process-scheduling">Process Scheduling</a>
                <a href="#sync">Synchronisation</a>
                <a href="#deadlock">Deadlock</a>
                <a href="#memory-mgmt">Memory Management</a>
                <a href="#io-storage-mgmt">IO Storage Management</a>
                <a href="#io-storage-mgmt">File Management</a>
                
            </div>
            <div class="text-wrapper">
                <div class="module-header">
                    <h1>Systems and Devices 2</h1>
                    <h2>Credits: 10</h2>
                    <h2><a href="https://www.york.ac.uk/students/studying/manage/programmes/module-catalogue/module/COM00021I/2020-21"> Module Description</a></h2>
                </div>
                <div class="module-topics-wrapper">
                    <div class="list-of-topics">
                        <h2>Topics:</h2>
                        <ul>
                            <li style="color: green">Operating Systems</li>
                            <li style="color: green">Processes</li>
                            <li style="color: green">Interprocess Communication</li>
                            <li style="color: rgb(196, 134, 0)">Threads</li>
                            <li style="color: red">Process Scheduling</li>
                            <li style="color: red">Synchronisation</li>
                            <li style="color: green">Deadlock</li>
                            <li style="color: green">Memory Management</li>
                            <li style="color: green">IO Storage Management</li>
                            <li style="color: red">File Management</li>
                        </ul>
                    </div>
                    <div class="operating-systems">
                        <h2 id="os">
                            Operating Systems
                        </h2>
                        <h3>What is an operating system?</h3>
                        <p>
                        Operating systems are <b>software</b> that manage computer's <b>hardware</b>. They act as an intermediary between the computer user and the computer hardware. They also provide the basis for 
                        application programs.
                        They provide:
                        <ul>
                            <li>Memory Management</li>
                            <li>I/O Management</li>
                            <li>Storage Management</li>
                        </ul>
                        </p>
                        <h3>Why do we need operating systems?</h3>
                        <p>
                            3 Reasons:
                            <ol>
                                <li><b>Convenience:</b> computer will be easier to use</li>
                                <li><b>Efficiency:</b> allows the computer system resources to be used efficiently</li>
                                <li><b>Abstraction</b> and interoperability: allows programmers/users flexibility to use different programming languages to application development, without having to worry about hardware architecture.</li>
                            </ol>
                            There are a number of com
                        </p>
                        <h3>Types of OS</h3>
                        <p>
                            There are many different operating systems to support different computing paradigms.
                            For example:
                            <ul>
                                <li>Free and open-source OS (e.g. UNIX)</li>
                                <li>Closed source or proprietry OS (e.g. Windows)</li>
                                <li>Hybrid approach (e.g. MacOS that has an open-source kernel but closed-source components too)</li>
                            </ul>
                        </p>
                        <h3>Operating System Services</h3>
                        <img src="images/degree-pictures/os-overview.png">
                        <p>
                           There are a number of different operating system services as shown above:
                           <ul>
                               <li>User interface: user interfaces allow the user to interact with the operating system through mouse/keyboard</li>
                               <li>Program execution: the system must be able to load a program into memory and run it. The programm must be able to end its execution</li>
                               <li>I/O operations: a running program may require I/O, that the operating system must allow the user to give.</li>
                               <li>File-system manipulation: programs will need to read and write files and directories, and be able to delete them, search for them and list information.</li>
                               <li>Communications: processes will often need to exchange information with other processes. This is done through <b>shared memory</b> or <b>message passing</b></li>
                               <li>Error detection: the OS needs to be detecting and correcting errors all the time.</li>
                           </ul>
                           Some of the services exist not for helping the user, but for ensuring efficcient operation of the system itself:
                           <ul>
                                <li>Resource allocation: when multiple processes run at the same time, resources must be allocated to each of them. The OS must manage this.</li>
                                <li>Accounting: we want to keep track of which programs use how and what kinds of computer resources. These statistics can be useful for administrators wishing to optimise comptuing services.</li>
                                <li>Protection and security: information owners in a shared system may want to control use of the information. The OS must therefore make sure all access to system resources is controlled.</li>
                            </ul>
                        </p>
                        <h3>System Calls</h3>
                        <p>
                            System calls provide an interface to the services made available by an operating system, for a program, usually as functions written in C and C++. For example, <b>cp in.txt out.txt</b> copies the input file 
                            to the output file. Application programmers will make use of Application Programming Interfaces <b>(APIs)</b>. These specify a set of functions that are available to the programmer, including
                            expected parameters and return values. A programmer accesses the API (e.g. POSIX API) through a library of code provided by the OS. For UNIX systems with programs written in C, the library is called
                            <b>libc.</b> So, APIs are sets of functions a programmer can access, to make use of system calls. APIs are accessed by a programmer using a library that the OS provides for that language.
                            
                        </p>
                        <img src="images/degree-pictures/system-call.png">
                        <p>
                            A <b>system call interface</b> is provided by the run-time nevironment (i.e. the softare needed for a program in a programming language to be executed). This serves as the link to system calls mmade available
                            by the operating system. Often, we need more information than just the identity of the system call. For example, to get an input file we need the name of the file. To pass parameters to the OS, usually registers
                            are used. However, if there are more parameters than registers, a block that stores the parameters will be passed as a parameter to a register.
                        </p>
                        <h3> Types of system calls</h3>
                        <ul>
                            <li><b>Process control:</b> These system calls include methods to create and terminate processes, load and execute them, get process attributes, wait for an event and signal for one, and allocate and free up
                            memory. </li>
                            <li><b>File management:</b>These include methods to create and delete files, open and close files, read, write and reposition files, and get file attributes.</li> 
                            <li><b>Device management:</b> These include methods to request and release devices, read and write to them, get device attributes, and logically attach, or detach, devices.</li>
                            <li><b>Information maintenance:</b> These include methods to get and set time/date, get and set system data, get processes, files or device attributes, and set them.</li>
                            <li><b>Communications:</b> These include methods to send and recieve messages, create and delete connections, transfer status information and attach or detach remote devices.</li>
                            <li><b>Protection:</b>These include methods such as getting and setting file permissions.</li>
                        </ul>
                        <h3>System utilities</h3>
                        <p>System utilities provide a convenient environment for program development and execution. Some are just interface to system calls (i.e. call a service), whereas as others are more complex.
                            They can be divided into these categories:
                            <ul>
                                <li>File management: allow the user to create, delete, copy, rename, print, dump, list, and generally manipulate files and directories.</li>
                                <li>Status information: utilities that allow the user to see system info such as date, time, available memory etc. Also for showing performance, logging
                                    and debugging information.
                                </li>
                                <li>File modification: text editors allow creation and mofification of files.</li>
                                <li>Programming language support (i.e. compilers, assemblers and debuggers)</li>
                                <li>Program loading and execution: many different loaders are used, and debugging systems for higher-level and machine language</li>
                                <li>Communications: allow the mechanism for creating virtual connections among processes, users, and computer systems</li>
                                <li>Background services: these launch at boot time and provide facilities sucha s disk checking, process scheduling, and error logging</li>
                            </ul>
                        </p>
                        <h3>Application Programs</h3>
                        <p>
                            Operating systems also come with <b>application programs</b> a lot of the time that are useful for solving common problems, but don't pertain to the system.
                            For example, web browsers or games. These aren't considered part of the OS generally.
                        </p>    

                        <p>Application prtograms compiled on one OS are not executable on other operating systems. This is because each operating system provides its own set 
                            of system calls. Sometimes, however, certain applications are available on multiple operating systems. This is for a few reasons:
                            <ul>
                                <li>The application can be written in an interpreted language (e.g. Python or Ruby) that has an interpreter available for multiple OSs.</li>
                                <li>The application can be written in a language that includes a virtual machine containing the running application e.g. Java. Java has an RTE that has been 
                                    developed for many operating systems, that can run the language on it.
                                </li>
                                <li>The application gets portred to each OS it will run on. This can be very time consuming and must be done for each new version.</li>
                            </ul>
                        </p>
                        <p>ABIs (Application Binary Interfaces) can be used to define how different components of binary code can interface for a given OS on a given archtecture. It sepcifices
                            low level details such as address width, and methods of passing parameters to system calls. They are specified for a given architecture 
                            and are therefore the architecture-level equivalent of an API. 
                        </p>
                        <h3>OS Design and Implementation</h3>
                        <p>
                            Design and implementation of OS is not 'solvable' but some approaches are more successful. There are certain 
                            <b>user goals</b> and <b>system goals</b> that need to be defined beforehand. User goals include OS convenience, how easy it is to learn, its reliableness,
                            safety and speed. System goals include how easy the OS is to design for, how easy it is to implement and maintain, and how flexible, reliable and error-free it is to do this.
                        </p>
                        <p>
                            We can use a <b>policy</b> and <b>mechanism</b> method of designign OSs. A policy is what needs to be done, and the mechanism is how to do it. The seperation of policy 
                            from mechanism is important as it allows flexibility if policy decisions are to be changed later.
                        </p>
                        <p>
                            In terms of implementation, there is lots of variation. Early OS's were implemented in assembly language, then system languages like Algol, and now C/C++. 
                            Usually, they are made of a mix of languages with the lowest in assembly, the main body in C and systems programs in C, C++ and scripting languages such as Python.
                            More high-level languages are easier to port to other hardware, but they are slower. Emulation allows for OSs to run on non-native hardware.
                        </p>
                        <h3>Operating System Structure</h3>
                        <h4>Monolithic Structure</h4>
                        <p>
                            Original UNIX had a monolithic structure. The OS consists of two seperablwe parts: systems programs and the kernel that consists of everything below the system-call interface and above 
                            the physical hardware. It provides the file system, CPU scheduling etc
                        </p>
                        <h4>Monolithic + Modular Structure</h4>
                        <p>
                            Linux has this structure, where parts of the OS are sepeated out into modules: applications, system-call interface, device drivers, and hardware
                        </p>
                        <h4>Layered Apporach</h4>
                        <p>
                            The OS is divided into a number of layers, that are built on top of lower layers. The bottom layer (layer 0) is the hardware, the highest (N) is the user interface.
                            With modularity, layers are selected such that each uses functions and services of only lower-level layers.
                        </p>
                        <h4>Microkernels</h4>
                        <p>
                            This approach moves as much from the kernel into user space as possible. <b>Mach</b> (that Mac OS kernel, Darwin, is partly based on) is an example of a microkernel OS.
                            Communication takes place between user modules using <b>message passing</b>. Has a number of benefits such as being easier to extend and port, and more reliable.
                            However, there is a performance overhead of user space to kernel space communication.
                        </p>
                        <h4>Modules</h4>
                        <p>
                            Many modern OSs implement <b>loadable kernel modules (LKMs)</b>. These use an object-oriented approach where each core component is seperate from each other. They talk to each other 
                            over known interfaces and each is loadable as needed within the kernel. Similar to layers but more flexible (Linux uses this, as mentioned earlier)
                        </p>
                        <h4>Hybrid Systems</h4>
                        <p>
                            The majority of modern OSs are not one pure model. Hybrid systems combine multiple approaches to address different needs. Windows is mostly monolithic, but also uses a microkernel apporach for different 
                            subsystem <b>personalities.</b> Apple Mac OS X is a hybrid. It is layered with an Aqua UI, with a kernel below it consisting of Mach microkernel and BSD Unix parts 
                            as well as dynamically loadable modules.

                        </p>
                        <h3>System Boot</h3>
                        <p>When the power is initalised on a system, execution starts at a fixed memory location. The OS must be made available to the hardware so the hardware can start it. A small piece of code 
                            called the <b>bootstrap loader (BIOS)</b> is stored in the <b>ROM</b>. This locates the kernel and loads it into memory, then starts it. The root file system is then mounted.      
                        </p>
                        <h3>OS Debugging</h3>
                        <p>
                            Debugging is finding and fixing errors (bugs). It is also called performance tuning. The OS generates <b>log files</b> that contain error information. The failure of an application 
                            can generate a core dump file, that captures the memory of the process. Failure of the OS itself can generate a crash dump file, containing kernel memory. Performance tuning 
                            can also optimize system performance. This is done through <b>trace listings</b> of activities that are recorded for analysis, and <b>profiling</b> that periodically samples the instruction 
                            pointer to look for trends.
                        </p>
                        <h4>Performance Tuning</h4>
                        <p>
                            In performance tuning, we can improve performance by removinng bottlenecks in the system. This means the OS must provide a means of computing and displaying measures of 
                            system behaviour. For example, Windows Task Manager.
                        </p>
                        <h4>Tracing</h4>
                        <p>
                            Tracing is where we collect data for a specific event, such as steps involved in a system call invocation. The tools include 'strace' - to trace system calls, 'gdb' - a source level debugger,
                            and 'perf' - a collection of Linux performance tools, to name a few.
                        </p>
                    </div>
                    <div class="processes">
                        <h2 id="process">
                            Processes
                        </h2>
                        <h3>What is a Process?</h3>
                        <p>
                            Processes are <b>programs in exeuction.</b> The execution of a process must progress in sequential fashion. There can therefore be no parallel execution of instructions of a single process.
                            An operating system executes a variety of programs that run as processes. Programs are static, whereas a process is dynamic (a program in execution). 
                        </p>
                        <p> 
                            Processes have multiple parts:
                            <ul>
                                <li>The program code, also called the <b>text section.</b></li>
                                <li>Current activity including <b>program counter</b> and processor registers.</li>
                                <li>A <b>stack</b> containing temporary data such as function parameters, return addresses and local variables.</li>
                                <li>The <b>data section</b> containing global variables.</li>
                                <li>The <b>heap</b> containing memory dynamically allocated during run time.</li>
                            </ul>
                            A program becomes a process when an executable file is loaded into memory. One program can be multiple processes.
                        </p>
                        <p>
                            As a process executes, it changes <b>state:</b>
                            <ul>
                                <li>New: the process is being created</li>
                                <li>Running: instructions are being executed</li>
                                <li>Waiting: the process is waiting for some event to occur</li>
                                <li>Ready: the process is waiting to be assigned to a processor</li>
                                <li>Terminated: the process has finished exeuction</li>
                            </ul>
                            It can be shown in a process flow diagram:
                        </p>
                        <img src="images/degree-pictures/process-flow.PNG"> 
                        <h3>Process Control Block</h3>
                        <p>
                            When a process is created, a process control block is created for that process that contains the information associated with it. For example:
                            <ul>
                                <li>Process state: running/waiting etc</li>
                                <li>Program counter: the location of instructions to next execute</li>
                                <li>CPU registers: contents of all process-centric registers</li>
                                <li>CPU scheduling information: priorities, scheduling queue pointers</li>
                                <li>Memory-management information: the memory allocated to the process</li>
                                <li>Accounting information: e.g. CPU used, clock time elapsed etc</li>
                                <li>I/O status information: I/O devices allocated to process etc</li>
                            </ul>
                        </p>
                        <h3>Threads</h3>
                        <p>
                            Multiple program counters per process would mean that multiple locations can execute at once. Essentially it means multiple <b>threads</b> of control. 
                            It would also mean that the PCB would need to store these multiple program counters.
                        </p>
                        <h3>Process Scheduling</h3>
                        <p>
                            The process scheduler selects between multiple processes for the next exeuction on a CPU core. The goal is to maximize CPU usage. It maintains scheduling queues
                            of processes. There is a <b>ready queue</b> with processes in main memory, waiting to execute, and a <b>wait queue</b> where processes are waiting for some event.
                            Processes will migrate across the various queues.
                        </p>
                        <h3>Context Switching</h3>
                        <p>
                            When the CPU switches to another process, the system must <b>save the state</b> of the old process and load the saved state for the new one via a <b>context switch</b>.
                            The context of a process is represented in the PCB. 
                        </p>
                        <p>
                            The OS must provide the mechanisms for process creation and termination. 
                        </p>
                        <h3>Process Creation</h3>
                        <p>
                            To create a process, a <b>parent</b> process must create a <b>child</b> process, which then create their own processes. This creates a <b>process tree</b>. 
                            Processes are generally identified and managed via a <b>pid</b> (process identifier)
                        </p>
                        <p>There are multiple options for processes sharing resources. They can either share all, some or none. There are also mutliple process execution options. Parents 
                            and children can execute concurrently, or the parent can wait for the child process to terminate.
                        </p>
                        <p>
                            For example, the C functions  fork() and exec() allow process creation. A parent process could call 'fork()', creating a new duplicate process (the child) that 
                            could run 'exec()' to replace the process' memory space with a new program. Meanwhile the parent process waits for the child to terminate. It can do this by checking 
                            the pid.
                        </p>
                        <img src="images/degree-pictures/fork-exec.PNG">
                        <h3>Process Termination</h3>
                        <p>
                            Processes exeucte their last statement and then ask the OS to delete them with the 'exit()' system call. The status is returned from child to parent with 'wait()'
                            and the resources are automatically deallocated. 
                        </p>
                        <p>
                            Parent processes can call 'abort()' to terminate child processes, in case the child has exceeded allocated resources, or the task is no longer required.
                            Also, an OS may not allow child processes to continue if the parent ones stop. Therefore, if a parent process terminates, all of its children must do so as well. 
                            This is called <b>cascading termination</b>, initiated by the OS. 
                        </p>
                        <p>
                            The wait() call returns the status information and pid of the terminated process. If no parent process invokes 'wait()' then the child process is a <b>zombie.</b>
                            If a parent process terminates without invoking 'wait()', the child is an <b>orphan.</b>
                        </p>
                    </div>
                    <div class="interprocess-communication">
                        <h2 id="ipc">
                            Interprocess Communication
                        </h2>
                        <p>
                            Processes in a system can be independent or cooperating. Processes may need to cooperate to share information, speedup computations, for modularity, or for convenience.
                            Cooperating processes need <b>interprocess communication</b>. There are two models of IPC:
                            <ul>
                                <li>Shared memory</li>
                                <li>Message passing</li>
                            </ul>
                        </p>
                        <h3>Producer-consumer problem</h3>
                        <p>
                            The paradigm for coopearting processes is in terms of a producer and a consumer. The producer process <em>produces</em> data that is <em>consumed</em>
                            by the consumer process. 
                            There are two variations:
                            <ul>
                                <li><b>unbounded buffer:</b> no practical limit on size of buffer. This means that the producer never waits, and the consumer waits when there is no buffer to consume.</li>
                                <li><b>bounded buffer:</b> assumes there is a fixed buffer size. The producer must wait if all buffers are full, and the consumer still waits if there is no buffer 
                                to consume.</li>
                            </ul>
                        </p>
                        <h3>Shared memory</h3>
                        <p>
                            Shared memory is where there is an area of memory among proccess that wish to communicate. The communication is under control of the user processes, and not the OS.
                            The issues are that actions from user processes will need to be synchronized when accessing shared memory. 
                        </p>
                        <h3>Message passing</h3>
                        <p>
                            Here, processes communicate with each other without resorting to shared variables. The IPC facility provides two operations:
                            <ul>
                                <li>send(message)</li>
                                <li>recieve(message)</li>
                            </ul>
                            Message size can be fixed or variable.
                        </p>
                        <p>
                            For two processes wishing to communicate, a <b>communication link</b> must be established between them. Messages are then exchanged through send/recieve. There are 
                            implemenetation issues in things like how links are established, how many processes can link, how many links can there be between processes, and capacity of links. 
                        </p>
                        <p>
                            Logically, communication links can be implemented through direct or indirect communication:
                            <ul>
                                <li>
                                    <b>Direct communication:</b> this is where processes must name each other explicitly, in the send and recieve commands. Links are established automatically,, and a link is 
                                    assoicated with exactly one pair of processes. There is exactly one link, and it may be unidirectional, but usually bi-directional.
                                </li>
                                <li>
                                    <b>Indirect communication:</b> this is where messages are directed and recieved from <b>mailboxes</b>, or ports. Each one has a unique id, and processes can communicate 
                                    only if they share a mailbox. Here, a link is established only if processes share a common mailbox. A link may be associated with many processes, and each pair of processes 
                                    can have multiple communication links. Links may be unidirectional or bi-directional.
                                    <br>
                                    The operations that can happen in indirect communication include the creation of new mailboxes, sending and recieving message through the mailbox, and the deletion 
                                    of a mailbox. The send and recieve operations will name the mailbox, instead of a process.
                                    <br>
                                    Problems could arise when there are multiple recievers of a mailbox. Who will get the message when two processes issue a recieve command 
                                    at the same time? Solutions could include allowing a link to be associated with at most two processes, or to allow only one process at a time to 
                                    execute a reccieve operation. 
                                </li>
                            </ul>
                            <h4>Synchronization</h4>
                            <p>
                                Message passing may be either blocking or non-blocking:
                                <ul>
                                    <li>
                                        <b>Blocking</b> is considered <b>synchronous.</b> A blocking send will block the sender from sending more messages until the message is recieved.
                                        A blocking recieve will block the reciever from recieving more messages until one is available. 
                                    </li>
                                    <li>
                                        <b>None-blocing</b> is considered <b>asynchronous.</b> A non-blocking send allows the sender to send a message and continue sending more. A non-blocking 
                                        recieve will mean the reciever can recieve messages whenever it wants, and may recieve a null message if none are available.
                                    </li>
                                </ul>
                                Different combinations of these are possible. If both send and recieve are blocking, it is called a <b>rendezvous.</b>
                            </p>
                            <h4>Pipes</h4>
                            <p>
                                Pipes act as a conduit, allowing two processes to communicate.
                                There are two types:
                                <ul>
                                    <li>
                                        <b>Ordinary pipes</b>. These cannot be accessed fromm outside the process that created it. Typically, a parent process creates a pipe and uses it to communicate 
                                        with a child process that it created. These allow communication in a standard produer-consumer style. The producer writes to one end of the pipe, the <b>write-end</b>
                                        and the consumer reads from the other end (the <b>read-end</b>). They are called <b>anonymous pipes</b> in Windows.
                                    </li>
                                    <li>
                                        <b>Named pipes.</b> These can be accessed <em>without</em> a parent-child relationship. These are therefore mmore powerful, as communication is bi-directional.
                                    </li>
                                </ul>
                            </p>
                        </p>

                    </div>
                    <div class="threads">
                        <h2 id="thread">
                            Threads
                        </h2>
                        Threads are...
                    </div>
                    <div class="process-scheduling">
                        <h2 id="process-scheduling">
                            Process Scheduling
                        </h2>
                        Process Scheduling is...
                    </div>
                    <div class="synchronisation">
                        <h2 id="sync">
                            Synchronisation
                        </h2>
                        Synchronisation is...
                    </div>
                    <div class="deadlock">
                        <h2 id="deadlock">
                            Deadlock
                        </h2>
                        Deadlock is what happens when threads both use and request 
                        <div class="tooltip">resources
                            <span class="tooltiptext"> These can include CPU cycles, files, and I/O devices. Mutex locks and semaphores are <b>also</b> resources</span>
                        </div> 
                        in a circular queue. Typically, a system table records whether each resource in a system is free or allocated. For each resource
                        that is allocated, the table also records the thread that it is allocated to. When a thread requests a resource that is currently allocated 
                        to another thread, it gets added to a queue of threads waiting for this resource. If every thread in the set is waiting to acquire resources
                        that must be released by another thread in the set then they are in a <b>deadlocked</b> state.
                        <br>
                        <img class="circular-queue" src="images/degree-pictures/circular-queue.png">
                        <br>
                        There are <b>four</b> conditions that must simultaneously
                        hold for deadlock to occur:
                        <ol>
                            <li><b>Mutual Exclusion</b></li>
                            At least <em>one</em> resource must be held in a nonsharable mode i.e. one thread at a time can use the resource. If another thread requests 
                            it then it (the requesting thread) must be delayed until the resource has been released.
                            <li><b>Hold and wait</b></li>
                            A thread must be holding at least one resource and waiting to acquire additional resources that are currently being held by other threads.
                            <li><b>No preemption</b></li>
                            Resources cannot be preempted (they can only be released voluntarily by the thread holding it when its completed its task)
                            <li><b>Circular wait</b></li>
                            A set {T0, T1, ..., Tn} of waiting threads must exist such that T0 is waiting for a resource held by T1, T1
                            is waiting for a resource held by T2, Tn-1 is waiting for Tn, and Tn is waiting for T0.
                        </ol>

                        <h3> Resource-Allocation Graph</h3>
                        We can describe deadlocks more precisely using a directed graph called a 
                        <b style="color: rgb(0, 102, 255);">system resource-allocation graph</b> consisting of vertices
                        V and edges E. V is split into T = {T1, T2, ..., Tn} and R = {R1, R2, ..., Rn}, the sets of threads and resource <em>types</em>
                        respectively. We represent a resource request as a directed edge from thread Ti, to resource type Rj (Ti -> Rj). A directed
                        edge from resource type Rj to thread Ti, represents that a resource has been allocated to the thread (Rj -> Ti).
                        These edges are called <b>request edges</b> and <b>assignment edges</b> respectively.
                        <br>
                        The number of resources in a resource type Rj are represented graphically as a small dot '<b>.</b>', as resource types
                        can have multiple resources available. Therefore an assigment edge must designate one of the dots.
                        <br>
                        A cycle <em>must</em> have a cycle to be in a deadlocked state. If there is a cycle, then the system <b>may or may not</b> be
                        in a deadlocked state. 

                        <h3>Handling Deadlocks</h3>
                        The deadlock problem can be handled by an operating system in one of three ways:
                        <ul>
                            <li>We can ignore the problem and pretend deadlocks never occur!</li>
                            Linux and Windows use this method and therefore leave it up to kernel and application developers to
                            handle them.
                            <br><br>
                            <li>We can use a protocol to prevent or avoid deadlocks</li>
                            Prevention -> ensure one of the necessary conditions do not hold
                            <br>
                            Avoidance -> the system is given additional information in advance concerning resources allocation so 
                            it can be decided whether or not threads should wait when requesting a resource.
                            <br><br>
                            <li>We can allows the system to enter a deadlocked state, detect it, and then recover</li>
                            Databases use this.
                        </ul>

                        <h3>Deadlock Prevention</h3>
                        We ensure one of the necessary conditions do not hold.
                        <ul>
                            <li>
                                <b>Mutual Exclusion:</b> we can use sharable resources, that do not require mutually exclusive access. For example,
                                read only files. However, this does not work as some resources are intrinisically nonsharable.
                            </li>
                            <li>
                                <b>Hold and Wait:</b> ensuring hold-and-wait does not occur, we must guarantee that when a thread requests a resource,
                                it does not hold any other resources. We could use a protocol where threads must be allocated all its resources before
                                it begins execution. However this is impractical due to the dynamic nature of requesting resources. This leads to low
                                resource utilization (since resources may be allocated but unused for a while). Also, starvation is possible as a thread 
                                may never be allocated its resources if they are popular and always allocated to other threads.
                            </li>
                            <li>
                                <b>No Preemption:</b> ensuring this does not hold means adopting a protocol like the following. If a thread is holding
                                resources and requests another that cannot be immediately allocated, then all resources the thread is currently holding 
                                are preempted. This means they are released, and added to the list of resources the thread is waiting for. Only when the
                                resources it had before, and the ones it now requires, are available can the thread be restarted.
                                <br>
                                This protocol is well suited to resources whose state can be easily saved and restored later, like CPU registers and
                                database transactions. It is not well-suited for resources like mutex locks and semaphores, where deadlocks occcur most
                                frequently.
                            </li>
                            <li>
                                <b>Circular Wait:</b> this conditions presents the best opportunity for preventing deadlocks. To do this, we must impose 
                                a total ordering of all resource types, and to require that each thread requests resources in an increasing order of
                                enumeration.
                                <br>
                                A function will give each resource type a unique natural number (F: R -> N). Resources will be requested in an increasing 
                                order of enumeration. So if Ri is requested, the thread can request Rj only if F(Rj) > F(Ri). If several instances of the 
                                same resources are needed, a <b>single</b> request for all of them must be issued.
                            </li>
                        </ul>
                        <h3>Deadlock Avoidance</h3>
                        If we can gather information about what resources that threads will use <em>ahead of time</em> then we can avoid a situation in 
                        which deadlock is possible. For example, one model requires that each thread declare the max number of resources of each type that
                        it may need.
                        <h4>Safe state</h4>
                        A state is <em>safe</em> if the system can allocate resources to each thread (up to its maximum) in some order and still avoid a 
                        deadlock. That is, there must exist a <b>safe sequence</b> of threads, where each threads request can be satsified by the currently
                        available resources plus the resources held by the ones before it.
                        <br><br> <b>Example:</b> <br><br>
                        Consider the following system, with twelve resources and three threads.
                        <br><br>
                        <table width="200px">
                            <tr>
                                <th>Thread</th>
                                <th>Maximum needs</th> 
                                <th>Current needs</th>
                              </tr>
                              <tr>
                                <td>T0</td>
                                <td>10</td>
                                <td>5</td>
                              </tr>
                              <tr>
                                <td>T1</td>
                                <td>4</td>
                                <td>2</td>
                              </tr>
                              <tr>
                                <td>T2</td>
                                <td>9</td>
                                <td>2</td>
                              </tr>
                        </table>
                        <br> Since 9 resources are currently being used (5+2+2), we have 3 (12-9) free resources. <br>
                        A t0, the system is in a safe state as there exists a sequence &lt;T1, T0, T2&gt;. We immediately can allocate T1 with the 2 exta
                        resources it needs (giving 3-2=1 free resource), until it releases all its resources after finishing (1+4) freeing up 5 resources.
                        These 5 resources can then be given to T0, which then frees up 10 in total. Then 7 resources can be given to T2 which then finishes.
                        <br><br>
                        Now we have defined a state where deadlock cannot exist, we can create avoidance algorithms that ensure the system cannot reach 
                        these states.
                        <h4>Banker's Algorithm</h4>
                        This algorithm is useful for when we have multiple instances of each resource type. A new type of edge is used, the <b>claim edge</b>. 
                        This edge is made before threads request resources, to indicate that the thread may request a certain resource in the future. It is 
                        represented by a dashed line. It gets converted into a request edge when the resource is requested, and back into a claim edge after
                        the resource is released.
                        <br> <br>
                        The banker's algorithm ensures that that when a new thread enters the system, it must declare the maximum number of instances 
                        of each resource type that it may need, and cannot exceed the total number of  resources in the system. When resources are requested, 
                        the system must determine whether the allocation of these resource will leave the system in a safe state.
                        <br> <br>
                        Several data structures must be maintained for implemenetation of the algorithm:
                        <ul>
                            <li>Available. A vector of length m that indicates the number of available resources of each type.</li>
                            <li>Max. An n x m matrix that defines the maximum demand of each thread. Max[i][j] = k means thread Ti may request at most
                                k instances of resource type Rj
                            </li>
                            <li>Allocation. An n x m matrix that defines the number of resource of each type currently allocated to each thread.</li>
                            <li>Need. An n x m matrix that indicates the remaining resource need of each thread. Need[i][j] = Max[i][j] - Allocation[i][j]</li>

                                                         
                        </ul>
                        The algorithm works as follows: <br>
                        <img class="safety-algorithm" src="images/degree-pictures/safety_algorithm.PNG"> <br>
                        Essentially, it allocates all threads as 'unfinished'. When it finds a thread that has needs that can be met (i.e. its resource needs
                        are available) then it gets allocated the resources, and these resources then get added to 'work'. If no thread can have its need met by
                        the available resources, the algorithm will decide the state is unsafe.
                        <br> <br>
                        We now need to use a resource-request algorithm, to determine whether requests can be safely granted: <br>
                        <img class="resource-request-algorithm" src="images/degree-pictures/resource-request.PNG"> <br>

                        <h3>Deadlock Detection</h3>
                        We can use algorithms to detect deadlocks, too. However, we must decide when we should invoke the detection algorith. This depends on two 
                        factors:
                        <ol>
                            <li>How <em>often</em> is a deadlock likely to occur?</li>
                            <li>How <em>many</em> threads will be affected by deadlock when it happens?</li>
                        </ol>
                        Generally, if deadlocks occur freqeuntly, then the detection algorithm ought to be invoked frequently to maximise
                        resource utilization. Checking for deadlocks at arbritrary points in time means we cannot tell which of the many deadlocked threads 'caused'
                        the deadlock.
                        <h3>Recovery from Deadlock</h3>
                        There are two options for breaking a deadlock. One is simply to abort one or more threads to break the circular wait. The other
                        is to preempt some resources fromm one or more of the deadlocked threads.
                        <h4>Process and thread termination </h4>
                        We can either abort all deadlocked processes (will definitely break the cycle, but at great expense), or abort one process at a time 
                        until the deadlock cycle is eliminated. The second method incurs a lot of overhead, as we need to run the deadlock-detection algorithm
                        every time we abort a process.
                        <br> <br>
                        In choosing which processes should be aborted, we want the ones that will incur the minimum cost. However <b>minimum cost</b> is subjective.
                        There are number of factors that may affect the choice of process such as:
                        <ul>
                            <li>Process priority</li>
                            <li>Length of time process has been computing, and how much longer is left</li>
                            <li>How many, and types, of resources the process has used (are they simple to preempt?)</li>
                            <li>How many mmore resources the process needs</li>
                            <li>How many processes will need to be terminated</li>
                        </ul>
                        <h4> Resource Preemption</h4>
                        To eliminate deadlocks using resource preemption, we succcessively preempt (remove and give to other) resources from processes and give 
                        these resources to other processes, until the deadlock cycle is broken. <br>
                        Three issues will need to be addressed:
                        <ol>
                            <li><b>Selecting a victim</b></li>
                            Which resources and which processes are to be pre-empted? Want to minimize cost (e.g. amount of time process has thus far consumed).
                            <li><b>Rollback</b></li>
                            What happens to the process? It cannot continue, so we must roll it back to some safe state and restart it from that state. Usually
                            it is easier to completely restart it, as determining the safe state is difficult. 
                            <li><b>Starvation</b></li>
                            How do we ensure starvation will not occur (resources not preempted from the same process every time)? The same victim may always be 
                            picked if using minimum cost. We must therefore ensure a victim can only be picked a small, finite number of times. We can therefore 
                            include the number of rollbacks in the cost factor.


                        </ol>
                    </div>
                    <div class="memory-management">
                        <h2 id="memory-mgmt">
                            Memory Management
                        </h2>
                        <h3>Basics:</h3>
                        <p>
                            The only way the CPU can retrieve memory is through main memory and the registers.
                            The CPU fetches instructions from main memory according to the value of the program counter.
                        </p>
                        <img src="images/degree-pictures/cpu-memory.png">
                        <p>
                            The memory in the processing cores, also called cache memory, is more difficult for the operating system to control.
                            The memory unit sees a stream of one of the following:
                            <ul>
                                <li>A read request + an address (e.g. LOAD memory location '20010' into register number '8'.</li>
                                <li>A write request + some data + an address (e.g. STORE contents of register '6' into memory location '1090'</li>
                            </ul>
                            The memory unit does not know <b>how</b> these addresses get generated. Accessing the register can be done within one
                            CPU clock. However, main memory access may take many cycles of the CPU clock. The processor must stall as it does not have
                            the required data to complete the instructions it is executing.
                            <br>
                            Therefore, we use <em>cache memory</em> to sit between main memory and the CPU registers to mitigate stalling.
                        </p>
                        <h3>Address Space</h3>
                        <p>
                            A (logical) address space is a range of addresses that an OS makes available to a process. The OS enforces memory protection.
                            That is, a process can only read and write within its address space. We must therefore seperate memory spaces which we do by
                            providing a range of legal addresses that the process may access. This is done through two registers, a base and a limit.
                            The <b>base register</b> holds the smallest legal physical memory address, and the <b>limit register</b> sepcifices the size
                            of the range. For example, if the base registers holds 300040 and the limit register is 120900, then the program ccan legally
                            access all addresses from 300040 through 420939 (base + limit) inclusive.
                            

                        </p>
                        <img src="images/degree-pictures/base-limit-registers.png"style="width:50%; height:50%;">
                        <p>
                            It is the CPUs job to check that every memory access generated in user mode is between the base and the base+limit for that
                            process.
                        </p>
                        <p>
                            There is also the issue of deciding where the program gets stored in main memory (after ist is loaded from secondary storage).
                            Could use address 0000 for the first physical address of a program and use that as reference, but this is impractical. Therefore
                            we need hardware and/or software support for memory management.
                        </p>
                        <p>
                            Addresses are represented in different ways at different stages of a program's life e.g. as a programmer you may use a variable name,
                            but the compiler will bind these symbolic addresses to relocatable addresses. The linker/loader will bind relocatable addresses to
                            absolute addresses.
                            The binding of instructions and data to memory addresses can be done at any step along the way:
                            <ul>
                                <li> 
                                    <b>Compile time</b>: if you know at compile time where the process will reside in memory, then <b>absolute code</b> can be generated.
                                    The code will need to be recompiled if the starting location changes at some later time.
                                </li>
                                <li>
                                    <b>Load time</b>: if it is not known at compile time where the process will reside in memory, then the compiler must generated
                                    <b>relocatable code</b>. Here, the final binding is delayed until load time. If the starting address changes, we need only to
                                    reload the user code to incorporate this changed value.
                                </li>
                                <li>
                                    <b>Execution time</b>: if the process can be moved during its execution from one memory segment to another, then binding must
                                    be delayed until run time. Special hardware is needed for this scheme to work. Most OSs use this method.
                                </li>
                            </ul>
                        </p>
                        <img src="images/degree-pictures/program-loading.png"style="width:50%; height:50%;">
                        <h3>Memory Management Unit</h3>
                        <p>
                            The Memory Management Unit (MMU) is a hardware device that, at runtime, maps logical addresses to physical addresses. The CPU will spit out a 
                            logical address, and the MMU will take that and give a physical address that can be used in physical memory by the OS.
                            The user program deals with logical addresses and will never see the real physical address. 
                            There are many ways to do these bindings, and have been worked on over time and became more and more complex.
                        </p>
                        <h3>Memory Management Methods</h3>
                        <h4>Contiguous Memory</h4>
                            A basic way of doing memory management is through 
                            <div class="tooltip">contiguous
                                <span class="tooltiptext"> This means 'in sequence' or 'next to each other'.</span>
                            </div> 
                            memory. This was where every job had access to the whole of the memory. This had advantages of being simple and a trivial address resolution. However,
                            the problems faced were numerous: only one job can run at a time, and the CPU was unused during I/O operations. Therefore there was no support for 
                            multi-programming (parallel processing).
                        <h4>Fixed Contiguous Partitions</h4>
                        <p>
                            This is where The OS assigns one partition per process, where the size of the partitions are defined at boot-up time and never change.
                            This way, there is protection against memory intrusion. The OS must be allocated its own partition. When a new process is started, the OS 
                            has to:
                            <ul>
                                <li>Determine the relevant partition</li>
                                <li>Determine the start address within the active partition</li>
                                <li>Resolve address: physical address = issued address + fixed base register</li>
                            </ul>
                            The problem is that it is hard to choose the correct partition sizes, and processes may require less space than available partitions. This is called fragmentation.
                            Lots of small jobs will prevent processes from being able to be ran, despite there being enough memory.
                        </p>
                        <h4>Dynamic Contiguous Partitions</h4>
                        <p>
                            To solve the problems of fragmentation, dynamic contiguous partitions were introduced. This is where partition size is dynamically selected when a job is loaded.
                            Addresses get resolved through a variable base register, instead of a fixed one (physical address = issued address + variable base register).
                            This alleviates the previous problems, however does not solve them completely. 
                            <br>
                            It introduces external fragmentation (as opposed to internal previously encountered). The OS must keep track of free partitions.
                        </p>
                        <img src="images/degree-pictures/external-fragmentation.PNG">
                        <p>
                            Here you can see that there are two free partitions in the memory, however a process that requests memory that is bigger than both partitions (but not bigger than
                            them combined) will not be allowed to run. This is because whilst there is available memory, it is in different partitions.
                        </p>
                        <h5>Partition allocation problem</h5>
                        <p>
                            If we want to satisfy a request of size <em>n</em> from a list of free partitions, there are a number of ways we can go about it:
                            <ul>
                                <li><b>First-fit</b>: allocate the first partition that is big enough</li>
                                <li>
                                    <b>Best-fit</b>: allocate the smallest partition that is big enough. This means the entire list of partitions must be searched, unless
                                    we maintain an ordered list.
                                </li>
                                <li><b>Worst-fit</b>: allocate the largest partition. The list must also be searched here.</li>
                            </ul>
                            There is no clear winner as the performance of each one depends on the request patterns.
                        </p>
                        <p>
                            External fragmentation can be mitigated by a compaction (or defragmentation) procedure. This requiores relocatable partitions, where the base 
                            register needs to be changed. The compaction alogirhm needs spare memory space to operate efficiently, as it moves small partitions out of the way 
                            before relocating large partitions. Compaction can not be performed whilst I/O is in progress involving memory that is being compacted.
                        </p>
                        <p>
                            As shown here, in the CPU there will be a relocation register, so logical addresses will be unchanged. The OS will apply the relocation 
                            though, and change the relocation register if the partition is moved elsewhere.
                        </p>
                        <img src="images/degree-pictures/relocation-register.PNG">
                        <h5>DCP - Swapping</h5>
                        <p>
                            A process can be <b>swapped</b> temmporarily out of memory to a backing store, and then brough back into memory for continued execution. This is done
                            because the total physical memory space of processes may actually exceed physical memory.
                        </p>
                        <p>
                            A <b>backing store</b> is a fast disk, large nough to accommodate binaries of all processes. A major part of swap time is transfer time, which is directly 
                            proportional to the amount of memory swapped.
                        </p>
                        <p>
                            Context switching time will include swapping if the next processes to be put on the CPU are not in memory. Therefore the context switch time 
                            will be very large. If we are swapping out a 100MB process and our hard disk has a transfer rate of 50MB/sec then there will be a swap out time of 2
                            seconds. This will be 4 seconds as we need to swap in a process of the same size.
                        </p>
                        <h4>Paging</h4>
                        <p>
                            The physical address space of a process can be noncontiguous: the process is allocated physical memory whenever it is available.
                            This avoids external fragmentation and the problem of having varying sized memory chunks.
                        </p>
                        <p>
                            Essentially, the physical memory is divided into fixed-size blocks called frames, and the logical memory is divided into blocks of of the same size called pages.
                            Interal fragmentation is still a minor issue here.
                        </p>
                        <p>
                            The OS must do a few things to manage paging. It must:
                            <ul>
                                <li>keep track of all free frames in memory</li>
                                <li>ensure that to run a program of size N pages it needs to find N free frames to load the program</li>
                                <li>
                                    set up a page table to translate logical to physical addresses, which is kept in memory:
                                    <ul>
                                        <li>Page-table base register (PTBR) points to the page table</li>
                                        <li>Page-table length register (PTLR) indicates the size of the page table</li>
                                    </ul>
                                </li>
                            </ul>
                            Therefore, there is significant overhead.
                        </p>
                        <h5>Address Resolution in Paging</h5>
                        <p>
                            If the logical address space is 2<sup>m</sup> and the page size is 2<sup>n</sup> then the address generated by the CPU is divided into:
                            <ul>
                                <li>Page number (p) which is used as an index into a page table which contains the base address of each page in physical memory.
                                    The size of p is "m-n" bits
                                </li>
                                <li>
                                    Page offset (d) which is combined with the base address to define the physical memory address that is sent to the memory unit. The size 
                                    of d is "n" bits.
                                </li>

                            </ul> 
                        </p>
                        <p>The diagram below shows how given p and d, a physical memory address can be resolved:</p>
                        <img src="images/degree-pictures/paging-address-resolution.PNG" style="width:50%; height:50%">
                        <p>Below is a badly drawn example of resolving an address when p=2 and d=2</p>
                        <img src="images/degree-pictures/page-resolution.PNG" style="width:50%; height:50%">
                        <p>
                            Another example below shows paging resolution when there are 4 bytes per page. In our logical memory '0' refers to the bytes abcd, in our page table, it refers
                            to '5' (the fifth page in our physical memory).
                        </p>
                        <img src="images/degree-pictures/paging-resolution-example.PNG">
                        <h5>Internal fragmentation in paging</h5>
                        <p>
                            There will be minor interal fragmentation in paging. This is because if a process requires an amount of memory that x pages can be used for, but 
                            the last page still has some free leftover memory then there is memory not being used. For example, a process requires 72,766 bytes. The page size is 
                            2048. Therefore there can be 35 pages of 2048 bytes plus 1 page of 1086. This leaves 962 (2048 - 1086) free bytes on the 36th page - internal fragmentation.

                        </p>
                        <p>
                            The worst case fragmentation will be 1 frame - 1 byte. On average it is 0.5 * frame size, however.
                            There is a performance trade-off with changing the page size. Reducing the page size minimizes internal fragmentation, but increasing it means there 
                            will be less pages needed, reducing the page table size and allowing a faster and simpler implementation of memory management.
                        </p>    
                        <h5>Performance issues in paging</h5>
                        <p>
                            If the page table is kept in main memory then ever data/instruction access requires two memory accesses (one for the page table and one for the data/instruction).
                            This problem can be solved by the use of a special fast-lookup hardware cache called <b>associative memory</b>, or a <b>translation look-aside buffers</b> (TLBs). 
                            The most common page/frames willl be stored here for quick access. 
                        </p>
                        <p>
                            The TLB is typically small (64 to 1024 entries). On a TLB miss (i.e. value not in the table), the value of the missed page-table and frame-number is loaded into 
                            the TLB for faster access next time that address is used. If there is no free TLB entry, replacement policies must be considered. 
                            Some entries can also be 'wired down' for permanent fast access.
                        </p>
                        <h4>Shared Pages</h4>
                        <p>
                            Shared pages are essentially shared code. One copy of read-only (reentrant) code is shared among processes (e.g. text editors, compilers, or window systems).
                            This is similar to multiple threads sharing the same process space. It is also useful for IPC if sharing of read-write pages is allowed. It eliminates the need
                            of replicating code in memory, as nothing is being written.
                        </p>
                        <p>
                            Code and data is kept private though, as each process keeps a seprate copy of the code and data.
                        </p>
                        <p>
                            Memory strucures for paging can get huge using straight-forward methods. For example, a 32-bit logical address space and a page size of 1KB (2<sup>10</sup>) would give
                            a page table of 4 million entries (2<sup>32</sup> / 2<sup>10</sup>). If each entry is 4 bytes, the page table is of size 16MB.
                            This can be costly, and we may not be able to allocate this contiguously in main memory
                            There are some solutions to this, such as exploiting hierarchy.
                        </p>
                        <h5>Hierarchical Page Tables</h5>
                        <p>
                            Here, we break up the logical address space into multiple page tables, such as a two-level page table. We then page the page table. 
                            As always, there is a trade-off. More layers means more time going through memory.
                        </p>
                        <img src="images/degree-pictures/page-table-hierarchy.PNG">
                        <p>
                            The page number will be divided into two numbers, for two page tables.
                        </p>
                        <img src="images/degree-pictures/hierarchical-page-tables.PNG" style="height:50%; width:50%">
                         <p>The ARM architecture is an example of one that uses a hierarchical page table structure.</p>
                    </div>
                    <div class="io-storage-management">
                        <h2 id="io-storage-mgmt">
                            I/O Storage Management
                        </h2>
                        <h3>I/O Management</h3>
                        <p>
                            The I/O subsystem is responsible for controlling all of the devices connected to a computer. It must:
                            <ul>
                                <li>Provide processes with a sufficiently simple interface</li>
                                <li>Take device characteristics into account to maximise performance and efficiency</li>
                            </ul>
                            There is a large variety of I/O devices such as storage (disk drives, non-volatile memory), communications (ethernet, wifi, bluetooth), and user interface (mouse, touch, keyboard, display, sound etc)
                        </p>
                        <h3>Device Drivers</h3>
                        <p>
                            These are low level software that interacts directly with the device hardware. They hide the hardware details from the higher levels of the OS and user applications, and are often developed by the hardware 
                            vendor. They track the status of the device and enforce access/allocation policies. There are a number of types:
                            <ul>
                                <li>Dedicated: each device is allocated to a single process</li>
                                <li>Shared: each device is shared between multiple processes</li>
                                <li>Virtual: hides sharing from processes</li>
                            </ul>
                        </p>
                        <h3>Devices</h3>
                        <p>
                            Devies usually have registers where the device driver places commands, addresses, and data to write. The devices read data from these registers after command execution.
                            There are a number of registers (data-in, data-out, status, and control) which are usually 1-4 bytes of fixed size, or a FIFO buffer.
                        </p>
                        <p>
                            Devices have addresses that are used by:
                            <ul>
                                <li>Direct I/O instructions</li>
                                <li>Memory mapped I/O: the device data and command registers are mapped to the processor address space</li>
                            </ul>
                        </p>
                        <p>
                            The I/O subsystem provides interfaces to access devices via the drivers. There are a number of communication mechanisms:
                            <ul>
                                <li>Polling and interrupts</li>
                                <li>Direct memory access (DMA)</li>
                                <li>Buffering</li>
                            </ul>
                        </p>
                        <h3>Polling</h3>
                        <p>
                            The CPU constantly 'polls' for the busy bit. For each byte of I/O: 
                            <ol>
                                <li>Read busy bit from status register until 0</li>
                                <li>Host sets the read or write bit and if write copies data into data-out register</li>
                                <li>Host sets command-ready bit</li>
                                <li>Controller sets busy bit, executes transfer</li>
                                <li>Controller clears busy bit, error bit, command-ready bit when trasnfer done</li>
                            </ol>
                            Step 1 is a busy-wait cycle to wait for I/O from device. This is reasonable if the device is fast, but inefficient if slow. The CPCU could switch to other tasks, but if a cycle is missed then 
                            data could be overwritten/lost. A way to prevent this is through <b>interrupts</b>.
                        </p>
                        <h4>Interrupts</h4>
                        <p>
                            Polling can happen in <b>3</b> exeuction cycles: read status, extract status bit, and branch if not zero. This can be more efficient if the bit is infrequently non-zero.
                        </p>
                        <p>
                            The CPU interrupt-request line can be triggered by an I/O device. It is checked by the processor after each instruction. The interrupt handler recieves the interrupts, and is able to 
                            ignore or delay some interrupts. An interrupt vector disptaches an interrupt to the correct handler. A context switch is needed here at the start and the end. We also need to chain interrupts if there 
                            are more than one devices at the same interrupt number.
                        </p>
                        <p>The following diagram shows how interrupts are essentially handled:</p>
                        <img src="images/degree-pictures/interrupt-handling.PNG" style="width: 75%;height:75">
                        <p>
                            Interrupts make more sense when I/O takes longer, because the context switches are expensive.

                        </p>
                        <h4>DMA</h4>
                        <p>
                            DMA, or Direct Memory Access, avoids the inefficiency of interrupts. It is used to avoid programmed I/O for large data movement, but in turn requires a DMA controller.
                            It bypasses the CPU to trasnfer data directly between the I/O device and memory.
                        </p>
                        <p>
                            The OS writes a DMA command block into memory, which contains:
                            <ul>
                                <li>Source and destination addresses</li>
                                <li>Read or write mode</li>
                                <li>The count of bytes</li>
                                <li>The location of the command block written to the DMA controller</li>
                            </ul>
                            The DMA controller issues an interrupt when it is ready. The followig diagram shows how it works:
                        </p>
                        <img src="images/degree-pictures/DMA-controller.PNG">
                        <p>
                            The DMA controller is a master on the system bus, and will compete with the CPU for control of that bus.
                        </p>
                        <h3>Storage Devices</h3>
                        <p>
                            The hierarchy of storage devices is driven by the performance and volatility of data. Data acess time includes the ready time (time to prepare and set up stroragem edia to read/write data at the appropriate
                            location, e.g. wind/rewind tape, rotate disk etc). It also includes transfer time: the time to read/write data from the media.
                        </p>
                        <p>
                            Difference devices can impose access latencies at different orders of magnitude. The OS should manag3e each of them appropriately and mediate transfers (e.g. buffering).
                        </p>
                        <h4>Teriary Storage</h4>
                        <p>
                            Tertiary storage is primarily used for backups and storage of infrequently-used data, and as a transfer medium between systems.
                            A magnetic tape is an example, with a large capacity (GB to TB) but very slow access time, as tape must be wound and rewound to a position.
                            However, once in place, it has reasonable transfer rates (>140MB/s)
                        </p>
                        <h4>Secondary Storage</h4>
                        <p>
                            This is non-volatile, high-capacity storage that supports swapping/paging. This includes magnetic disks (HDDs), SSDs, and Redundant Arrays of Independent Disks (RAIDs).
                        </p>
                        <p>
                            Magnetic disks are made of n disks (with 2n sides), where each side is divided into circular tracks, and each track into sectors. One cyclinder is a set of tracks at the same position on all sides. Access time
                            is the seek time (disk head movement) + search time (rotational delay) + transfer time.
                        </p>
                        <p>
                            Other non-volatile memory, such as flash memory, have no mechanical components; only a cointroller and NAND-based flash memory integrated circuits. These are more reliable than HDs (no moving parts), can be faster (no 
                            seek time or latency) and consume less power. They are also therefore more expensive per MB, lower capacity (generally) and may have a shorter lifespan.
                        </p>
                        <h4>RAID<h4>
                        <p>
                            These are sets of physical disks viewed as a single logic unit by the OS. They allow simultaneous access to multiple drives, which gives increased I/O performance and improved data recovery in case of failure. Data is 
                            divided into segments called stripes, which are distributed across the disks in the array. RAIDs can be classified as level 0, 1, .., 6 which indicate different approaches to 
                            data redundancy and error correction methods.
                        </p>
                        <h4>RAID 0</h4>
                        <p>
                            In RAID 0, data is divided in segments that are stored across the disks in the array. There is a minimum of 2 disks. The read speed-up is proportional to the number of disks in the array, since distinct data can be read from
                            different disks simultaneously. The write speed-up is also proportional as distinct data can be written to different disks simultaneously.
                        </p>
                        <img src="images/degree-pictures/raid0.PNG">
                        <p>
                            However, if one disk fails, there is no redundancy, and therefore no fault tolerance. Since reliability is inversely proportional to the number of disks, a RAID 0 will be more 
                            vulnerable to faults than a single hard disk. Also, the storage space added to the array by each disk is limited to the size of the smallest disk. e.g. if you striped a 120GB disk 
                            together with a 100GB disk, the capacity will only be 100 + 100 = 200GB.
                        </p>
                        <h4>RAID 1</h4>
                        <p>
                            RAID 1 has full redundancy (called mirroring) where data is copied in all disks of the array. Like RAID 0, there must be a minimumm of 2 disks. It tolerates faults on up to N-1 disks (i.e. would tolerate 4 faulty disks on a 
                            5 disk set-up). The read speed-up is proportional to the number of disks in the array, for the same reasons as RAID 0, but there is no write speed-up since writes have to be done on all disks. It also has very low space 
                            efficiency (1/N).
                        </p>
                        <img src="images/degree-pictures/raid1.PNG">
                        <h4>RAID 2 (3,4)</h4>
                        <p>
                            In these levels, there is bit-level striping, dedicated Hamming (7,4) parity where there is four data bits plus three parity bits. There is a minimum of 3 disks, and tolerates fault in just one disk.
                            These levels are now obsolete, however, as it is more complex than built-in error correction in modern hard disks.
                        </p>
                        <h4>RAID 5</h4>
                        <p>
                            RAID 5 distributes parity over multiple disks. There must be 3 disks minimum, and tolerates fault in one disk. Writes are costly operations, but it is still widely used.
                        </p>
                        <img src="images/degree-pictures/raid5.PNG">
                        <h4>RAID 6</h4>
                        <p>
                            RAID 6 doubly distributes parity, with a minimum of 4 disks and fault tolerance in two. Like RAID 5, it is also widely used (but costly writes still).
                        </p>
                        <img src="images/degree-pictures/raid6.PNG">
                        <h3>Storage Management</h3>
                        <p>
                            Multiple requests need to be handled concurrently, for example several programs may request storage operations at the samet time. We therefore need policies for servicing disk requests. An I/O scheduler handles this 
                            and decides which requests should be next executed, and allows for prioritised requests. Specific devices may require dedicated scheduling policies, for example to minimise seek time in magnetic disks, or avoid redundant 
                            writes in non-volatile memory. 
                        </p>
                        <h4>Disk Management</h4>
                        <p>
                            The OS allows for disk formatting on the physical level (using disk sectors and checksums), and the logical level (using directory-trees and mapping free and allocated space). The OS also allows for partitions (logical disks on 
                            one physical one), a boot block (inital bootable code at a known sector), bad blocks (permanently corrupted blocks), and defragmentation (rearranging sectors used by files to be contiguous).
                        </p>
                    </div>
                    <div class="file-management">
                        <h2 id="file-mgmt">
                            File Management
                        </h2>
                        File Management is...
                    </div>
                </div>
                
            </div>

        </div>
    </div>
</body>
</html>